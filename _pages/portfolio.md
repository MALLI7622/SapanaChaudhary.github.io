---
layout: archive
title: "Projects"
permalink: /portfolio/
author_profile: true
---

<details>
  <summary>Reinforcement Learning</summary>
  <p>

### Smoother Imitation with Lipschitz Costs
  * With Akshat Dave, Balaraman Ravindran
  * Accepted for Poster Presentation at NIPS DRL Symposium 2017.
  * Generative Adversarial Imitation Learning (GAIL) presents a specific approach to the
    task of imitating an expert by jointly modelling the environment’s reinforcement signal
    and the imitating agent’s policy. GAIL provides state-of-the-art results in imitating
    complex behaviours in large, high dimensional environments. However, the algorithm
    often suffers from instability during the training and high variance in the returns and
    the trajectories. In this work, we propose a GAIL-like framework for learning smoother
    imitation and achieving consistently meaningful learning gradients. The learned policyachieves better performance than          the existing methods in terms of closeness to the expert trajectories and the value of the true returns. We propose metrics     to evaluate for the better imitation of the expert and the smoothness of the learned policies. We
empirically evaluate the algorithm on simulated continuous control tasks from MuJoCo.

-----------------------------------------------------------------------------------
  ### On the Analysis of Lipschitz Smoothness of Costs for Learning Smooth Policies

-----------------------------------------------------------------------------------
  ### Variance Reduction in Policy Gradients through Smooth Costs  

-----------------------------------------------------------------------------------
  ### Learning Domain-Invariant Policies in RL
</p>
</details>


<details><summary>Optimization</summary>
  
### Analyzing and Quantifying Missing Modes in GANs

* With Rahul Vallivel, Mitesh Khapra, Balaraman Ravindran
* In this work, we analyse various issues with the Generative Adversarial Network (GAN)
  architecture, training, the loss function and the training algorithm. We run an
  exploratory set of experiments on mixture of Gaussians, MNIST and CelebA to
  understand what goes wrong and why. We concentrate specifically on the problem of
  missing modes in generative densities modelled by GANs. We observe that a difference
  in loss function of GANs leads to
  * Different learning rates that need to be used for model training
  * Difference in the amount of true distribution that can be recovered.
  * We also run experiments to measure input covariate shift in GANs, using gradient
    of the discriminator with respect to the inputs to quantify the same.

### Localization of Cellular Networks 


### Spectrum Cartography using Wireless Cellular Data


### James-Stein Estimator
Studied JS-Estimator to perform biased estimation for orthogonal frequency division
multiplexing in the Wireless Communications course.

### Report on 'Constrained convex minimization via model based excessive gap'
As a part of Term Paper Presentation in the course on Algorithms for Convex Optimization,
reviewed paper on ”Constrained convex minimization via model-based excessive gap (NIPS
2014)”

### Natural Gradient Descent for Neural Networks
</details>


<details><summary>Machine Learning</summary>

### Multi-class classification of 100 class data 
  
This project was done as a part of course on Introduction to Machine Learning. The train data
provided corresponded to a 100 class classification problem. We had to perform the
classification task resulting in the best mean F1-measure for the 100 classes.

</details>


<details><summary>Systems</summary>
  
### Automatic Vehicle Speed Reduction using GPS


### RFID based Localization 


### Wireless Energy Meter Module Development 


### WiFi Channel Modelling 


</details>
